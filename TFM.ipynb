{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a435e20b",
   "metadata": {},
   "source": [
    "# PIPELINE\n",
    "\n",
    "Carga correctamente los 4 archivos del TRAIN_NEW.\n",
    "\n",
    "Unifica la base mediante el ID participant_id.\n",
    "\n",
    "Separa variables categóricas, cuantitativas y fMRI.\n",
    "\n",
    "Escala las numéricas con StandardScaler.\n",
    "\n",
    "Codifica las categóricas con OneHotEncoder.\n",
    "\n",
    "Divide el dataset en entrenamiento/test estratificado.\n",
    "\n",
    "Produce matrices listas para entrenar modelos ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98320478",
   "metadata": {},
   "source": [
    "Siempre que trabajamos con conectomas fMRI (~20.000 features), los pasos estándar son:\n",
    "\n",
    "Imputación\n",
    "\n",
    "Escalado\n",
    "\n",
    "Reducción (PCA)\n",
    "\n",
    "Clasificador (RF, SVM, XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880a9d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\".\", topdown=True):\n",
    "    for d in dirs:\n",
    "        print(\"DIR:\", os.path.join(root, d))\n",
    "    for f in files:\n",
    "        print(\"FILE:\", os.path.join(root, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa26c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# IMPORTS FOR PREPROCESSING & MODELING\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a614c6c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cols: ['participant_id', 'Basic_Demos_Enroll_Year', 'Basic_Demos_Study_Site', 'PreInt_Demos_Fam_Child_Ethnicity', 'PreInt_Demos_Fam_Child_Race', 'MRI_Track_Scan_Location', 'Barratt_Barratt_P1_Edu', 'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ']\n",
      "Quantitative cols: ['participant_id', 'EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP', 'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems', 'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems', 'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact', 'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing', 'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan']\n",
      "Connectome cols: ['participant_id', '0throw_1thcolumn', '0throw_2thcolumn', '0throw_3thcolumn', '0throw_4thcolumn', '0throw_5thcolumn', '0throw_6thcolumn', '0throw_7thcolumn', '0throw_8thcolumn', '0throw_9thcolumn']\n",
      "Solutions cols: ['participant_id', 'ADHD_Outcome', 'Sex_F']\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. LOAD FILES\n",
    "# ===============================\n",
    "\n",
    "\n",
    "base_path = \"TRAIN_NEW/\"\n",
    "\n",
    "path_cat   = base_path + \"TRAIN_CATEGORICAL_METADATA_new.xlsx\"\n",
    "path_quant = base_path + \"TRAIN_QUANTITATIVE_METADATA_new.xlsx\"\n",
    "path_conn  = base_path + \"TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv\"\n",
    "path_sol   = base_path + \"TRAINING_SOLUTIONS.xlsx\"\n",
    "\n",
    "# Load the datasets\n",
    "df_cat   = pd.read_excel(path_cat)\n",
    "df_quant = pd.read_excel(path_quant)\n",
    "df_conn  = pd.read_csv(path_conn)\n",
    "df_sol   = pd.read_excel(path_sol)\n",
    "\n",
    "# ID column\n",
    "id_col = \"participant_id\"\n",
    "\n",
    "print(\"Categorical cols:\", df_cat.columns.tolist())\n",
    "print(\"Quantitative cols:\", df_quant.columns.tolist())\n",
    "print(\"Connectome cols:\", df_conn.columns[:10].tolist())\n",
    "print(\"Solutions cols:\", df_sol.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0b3789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged shape: (1213, 19930)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 2. MERGE TABLES ON participant_id\n",
    "# ===============================\n",
    "\n",
    "# Merge metadata\n",
    "df_meta = df_cat.merge(df_quant, on=id_col, how=\"inner\")\n",
    "\n",
    "# Merge connectome (fMRI)\n",
    "df_all = df_meta.merge(df_conn, on=id_col, how=\"inner\")\n",
    "\n",
    "# Merge targets\n",
    "df_all = df_all.merge(df_sol, on=id_col, how=\"inner\")\n",
    "\n",
    "print(\"Final merged shape:\", df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77729ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3. SEPARATE FEATURES AND TARGET\n",
    "# ===============================\n",
    "\n",
    "target_col = \"ADHD_Outcome\"\n",
    "sex_col = \"Sex_F\"   # keep for later analysis\n",
    "\n",
    "y = df_all[target_col]\n",
    "sex = df_all[sex_col]\n",
    "\n",
    "# Drop ID + target columns from features\n",
    "X = df_all.drop(columns=[id_col, target_col, sex_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e582c323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: 9\n",
      "Numerical features: 19918\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 4. IDENTIFY CATEGORICAL / NUMERICAL COLUMNS\n",
    "# ===============================\n",
    "\n",
    "cat_cols = df_cat.columns.drop(id_col).tolist()\n",
    "num_cols_meta = df_quant.columns.drop(id_col).tolist()\n",
    "\n",
    "# Connectome columns = all columns in df_conn except ID\n",
    "conn_cols = [c for c in df_conn.columns if c != id_col]\n",
    "\n",
    "num_cols = num_cols_meta + conn_cols  # all numerical data\n",
    "\n",
    "print(\"Categorical features:\", len(cat_cols))\n",
    "print(\"Numerical features:\", len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9540c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 5. PREPROCESSING PIPELINE (with imputation)\n",
    "# ===============================\n",
    "\n",
    "# Pipeline para numéricas: imputa medianas + escala\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline para categóricas: imputa la moda + one-hot\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037e3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 6. TRAIN/TEST SPLIT\n",
    "# ===============================\n",
    "\n",
    "X_train, X_test, y_train, y_test, sex_train, sex_test = train_test_split(\n",
    "    X, y, sex,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30fd0793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train processed shape: (970, 19980)\n",
      "Test processed shape: (243, 19980)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 7. FIT TRANSFORMER ON TRAIN \n",
    "# ===============================\n",
    "\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Train processed shape:\", X_train_proc.shape)\n",
    "print(\"Test processed shape:\", X_test_proc.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7846d",
   "metadata": {},
   "source": [
    "# Random Forest Classifier (Analisis preliminar)\n",
    "\n",
    "Funciona bien con alta dimensionalidad (20.000+ features).\n",
    "\n",
    "Tolera ruido (muy común en conectomas).\n",
    "\n",
    "No requiere supuestos de distribución.\n",
    "\n",
    "Proporciona una primera métrica clara.\n",
    "\n",
    "Permite obtener una importancia de features (aunque luego lo mejora SHAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b24f6b",
   "metadata": {},
   "source": [
    "1.Entrenar RandomForest \n",
    "\n",
    "2.Obtener:\n",
    "F1-score\n",
    "Precision\n",
    "Recall\n",
    "Matriz de confusión\n",
    "\n",
    "3.Ver si existe sesgo por sexo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a88cb8",
   "metadata": {},
   "source": [
    "# Analisis preliminar de random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4fbded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===============================\n",
    "# 8. RANDOM FOREST BASELINE\n",
    "# ===============================\n",
    "\n",
    "# Modelo baseline (ajustes simples)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,       # Número de árboles\n",
    "    max_depth=None,        # Dejar crecer completamente\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"   # MUY IMPORTANTE para TDAH (clase desbalanceada)\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "rf.fit(X_train_proc, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred = rf.predict(X_test_proc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b414db7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        77\n",
      "           1       0.68      1.00      0.81       166\n",
      "\n",
      "    accuracy                           0.68       243\n",
      "   macro avg       0.34      0.50      0.41       243\n",
      "weighted avg       0.47      0.68      0.55       243\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[  0  77]\n",
      " [  0 166]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlessiaDerossi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AlessiaDerossi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AlessiaDerossi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 9. MÉTRICAS DE RENDIMIENTO\n",
    "# ===============================\n",
    "\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10f7c1",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba68cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after PCA: (970, 300) (243, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Probamos 300 componentes (ajustable)\n",
    "pca = PCA(n_components=300, random_state=42)\n",
    "\n",
    "# Ajustamos PCA solo con train\n",
    "X_train_pca = pca.fit_transform(X_train_proc)\n",
    "X_test_pca = pca.transform(X_test_proc)\n",
    "\n",
    "print(\"Shape after PCA:\", X_train_pca.shape, X_test_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b6ab5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varianza explicada total: 0.7242815519697952\n"
     ]
    }
   ],
   "source": [
    "print(\"Varianza explicada total:\", np.sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb4551",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Hecho el PCA (250 componentes) y las variables:\n",
    "\n",
    "X_train_pca, X_test_pca\n",
    "\n",
    "y_train, y_test\n",
    "\n",
    "Definir el modelo, entrenar y evaluar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f52c23e6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/30/7d/41847e45ff075f3636c95d1000e0b75189aed4f1ae18c36812575bb42b4b/xgboost-3.1.2-py3-none-win_amd64.whl.metadata\n",
      "  Downloading xgboost-3.1.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from xgboost) (1.11.1)\n",
      "Downloading xgboost-3.1.2-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/72.0 MB 16.1 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 2.6/72.0 MB 27.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 4.2/72.0 MB 30.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 7.2/72.0 MB 38.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 9.6/72.0 MB 41.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 12.0/72.0 MB 46.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 13.5/72.0 MB 43.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 15.0/72.0 MB 46.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 16.8/72.0 MB 40.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 17.6/72.0 MB 36.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 19.1/72.0 MB 34.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 20.8/72.0 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 22.6/72.0 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 24.4/72.0 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 25.9/72.0 MB 32.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 27.6/72.0 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 29.7/72.0 MB 36.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 31.4/72.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 34.0/72.0 MB 38.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 36.1/72.0 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 38.0/72.0 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 39.9/72.0 MB 43.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 41.5/72.0 MB 43.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 43.1/72.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 45.0/72.0 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 48.3/72.0 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 50.9/72.0 MB 50.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 53.0/72.0 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 54.4/72.0 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 56.1/72.0 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 57.2/72.0 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 59.1/72.0 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 61.2/72.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 63.5/72.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 66.0/72.0 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 68.8/72.0 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  72.0/72.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  72.0/72.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  72.0/72.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  72.0/72.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  72.0/72.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 20.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1ec4e1a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.13      0.19        77\n",
      "           1       0.69      0.89      0.77       166\n",
      "\n",
      "    accuracy                           0.65       243\n",
      "   macro avg       0.52      0.51      0.48       243\n",
      "weighted avg       0.58      0.65      0.59       243\n",
      "\n",
      "=== XGBoost Confusion Matrix ===\n",
      "[[ 10  67]\n",
      " [ 19 147]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ===============================\n",
    "# XGBoost con PCA ( 300 componentes)\n",
    "# ===============================\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test_pca)\n",
    "\n",
    "print(\"=== XGBoost Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"=== XGBoost Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5cd9f",
   "metadata": {},
   "source": [
    "# Interpretacion por sexo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b50e8a26",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost - Mujeres (F) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.16      0.23        31\n",
      "           1       0.62      0.86      0.72        49\n",
      "\n",
      "    accuracy                           0.59        80\n",
      "   macro avg       0.52      0.51      0.48        80\n",
      "weighted avg       0.54      0.59      0.53        80\n",
      "\n",
      "Matriz de confusión F:\n",
      "[[ 5 26]\n",
      " [ 7 42]]\n",
      "\n",
      "=== XGBoost - Varones (M) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.11      0.16        46\n",
      "           1       0.72      0.90      0.80       117\n",
      "\n",
      "    accuracy                           0.67       163\n",
      "   macro avg       0.51      0.50      0.48       163\n",
      "weighted avg       0.60      0.67      0.62       163\n",
      "\n",
      "Matriz de confusión M:\n",
      "[[  5  41]\n",
      " [ 12 105]]\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Evaluación por sexo\n",
    "# ===============================\n",
    "\n",
    "# Índices de niñas y niños en el set de test\n",
    "idx_female = (sex_test == 1)\n",
    "idx_male = (sex_test == 0)\n",
    "\n",
    "# Filtrar verdaderos y predichos por sexo\n",
    "y_test_f = y_test[idx_female]\n",
    "y_pred_f = y_pred_xgb[idx_female]\n",
    "\n",
    "y_test_m = y_test[idx_male]\n",
    "y_pred_m = y_pred_xgb[idx_male]\n",
    "\n",
    "print(\"=== XGBoost - Mujeres (F) ===\")\n",
    "print(classification_report(y_test_f, y_pred_f))\n",
    "print(\"Matriz de confusión F:\")\n",
    "print(confusion_matrix(y_test_f, y_pred_f))\n",
    "\n",
    "print(\"\\n=== XGBoost - Varones (M) ===\")\n",
    "print(classification_report(y_test_m, y_pred_m))\n",
    "print(\"Matriz de confusión M:\")\n",
    "print(confusion_matrix(y_test_m, y_pred_m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b3ae1",
   "metadata": {},
   "source": [
    "# PCA + SVM (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b58926e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PIPELINE PCA + SVM (FINAL) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.35      0.34        77\n",
      "           1       0.69      0.66      0.67       166\n",
      "\n",
      "    accuracy                           0.56       243\n",
      "   macro avg       0.50      0.50      0.50       243\n",
      "weighted avg       0.57      0.56      0.56       243\n",
      "\n",
      "[[ 27  50]\n",
      " [ 57 109]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ===============================\n",
    "# PIPELINE FINAL: PCA + SVM (RBF)\n",
    "# ===============================\n",
    "\n",
    "pca_svm = Pipeline(steps=[\n",
    "    (\"pca\", PCA(n_components=300, random_state=42)),\n",
    "    (\"svm\", SVC(kernel=\"rbf\",\n",
    "                class_weight=\"balanced\",\n",
    "                probability=True,\n",
    "                random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenamiento\n",
    "pca_svm.fit(X_train_proc, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred_pca_svm = pca_svm.predict(X_test_proc)\n",
    "\n",
    "print(\"=== PIPELINE PCA + SVM (FINAL) ===\")\n",
    "print(classification_report(y_test, y_pred_pca_svm))\n",
    "print(confusion_matrix(y_test, y_pred_pca_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c44b04b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PCA+SVM - Mujeres ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.55      0.52        31\n",
      "           1       0.69      0.63      0.66        49\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.59      0.59      0.59        80\n",
      "weighted avg       0.61      0.60      0.60        80\n",
      "\n",
      "[[17 14]\n",
      " [18 31]]\n",
      "\n",
      "=== PCA+SVM - Varones ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.22      0.21        46\n",
      "           1       0.68      0.67      0.68       117\n",
      "\n",
      "    accuracy                           0.54       163\n",
      "   macro avg       0.44      0.44      0.44       163\n",
      "weighted avg       0.55      0.54      0.54       163\n",
      "\n",
      "[[10 36]\n",
      " [39 78]]\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# EVALUACIÓN POR SEXO: Modelo Final PCA+SVM\n",
    "# ===============================\n",
    "\n",
    "idx_female = (sex_test == 1)\n",
    "idx_male   = (sex_test == 0)\n",
    "\n",
    "y_test_f = y_test[idx_female]\n",
    "y_pred_f = y_pred_pca_svm[idx_female]\n",
    "\n",
    "y_test_m = y_test[idx_male]\n",
    "y_pred_m = y_pred_pca_svm[idx_male]\n",
    "\n",
    "print(\"=== PCA+SVM - Mujeres ===\")\n",
    "print(classification_report(y_test_f, y_pred_f))\n",
    "print(confusion_matrix(y_test_f, y_pred_f))\n",
    "\n",
    "print(\"\\n=== PCA+SVM - Varones ===\")\n",
    "print(classification_report(y_test_m, y_pred_m))\n",
    "print(confusion_matrix(y_test_m, y_pred_m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0eeb9a",
   "metadata": {},
   "source": [
    "# Explicabilidad con SHAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "460a1f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for shap from https://files.pythonhosted.org/packages/77/03/58e199cf59056d68b4a227ce4b2b09eeb0c9bd1d002b9e28fb574eed6200/shap-0.50.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading shap-0.50.0-cp311-cp311-win_amd64.whl.metadata (25 kB)\n",
      "Collecting numpy>=2 (from shap)\n",
      "  Obtaining dependency information for numpy>=2 from https://files.pythonhosted.org/packages/aa/44/9fe81ae1dcc29c531843852e2874080dc441338574ccc4306b39e2ff6e59/numpy-2.3.5-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numpy-2.3.5-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.9/60.9 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from shap) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from shap) (1.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from shap) (2.0.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from shap) (4.65.0)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from shap) (23.1)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Obtaining dependency information for slicer==0.0.8 from https://files.pythonhosted.org/packages/63/81/9ef641ff4e12cbcca30e54e72fb0951a2ba195d0cda0ba4100e532d929db/slicer-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from shap) (0.57.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from shap) (4.7.1)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from numba>=0.54->shap) (0.40.0)\n",
      "INFO: pip is looking at multiple versions of numba to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Obtaining dependency information for numba>=0.54 from https://files.pythonhosted.org/packages/5b/82/9d425c2f20d9f0a37f7cb955945a553a00fa06a2b025856c3550227c5543/numba-0.62.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numba-0.62.1-cp311-cp311-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.54->shap)\n",
      "  Obtaining dependency information for llvmlite<0.46,>=0.45.0dev0 from https://files.pythonhosted.org/packages/a4/56/4c0d503fe03bac820ecdeb14590cf9a248e120f483bcd5c009f2534f23f0/llvmlite-0.45.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading llvmlite-0.45.1-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from pandas->shap) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (2.2.0)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy (from shap)\n",
      "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/f1/d0/22ec7036ba0b0a35bccb7f25ab407382ed34af0b111475eb301c16f8a2e5/scipy-1.16.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alessiaderossi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Downloading shap-0.50.0-cp311-cp311-win_amd64.whl (547 kB)\n",
      "   ---------------------------------------- 0.0/548.0 kB ? eta -:--:--\n",
      "   --------------------------------------  542.7/548.0 kB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 548.0/548.0 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading numba-0.62.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 2.3/2.7 MB 49.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 29.1 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.5-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "   ---------------------------------------- 0.0/13.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/13.1 MB 30.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.6/13.1 MB 33.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.1/13.1 MB 37.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.2/13.1 MB 39.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/13.1 MB 38.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.1/13.1 MB 42.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.1 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.1 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.1/13.1 MB 34.5 MB/s eta 0:00:00\n",
      "Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "   ---------------------------------------- 0.0/38.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/38.7 MB 39.4 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 2.9/38.7 MB 37.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 5.4/38.7 MB 48.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 7.1/38.7 MB 45.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 9.4/38.7 MB 46.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 13.4/38.7 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 16.2/38.7 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 19.4/38.7 MB 73.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 22.8/38.7 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 26.9/38.7 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.7/38.7 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.5/38.7 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.9/38.7 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/38.7 MB 65.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/38.7 MB 65.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/38.7 MB 65.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/38.7 MB 65.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.7/38.7 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.45.1-cp311-cp311-win_amd64.whl (38.1 MB)\n",
      "   ---------------------------------------- 0.0/38.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.5/38.1 MB 73.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 5.6/38.1 MB 72.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 8.2/38.1 MB 65.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 11.3/38.1 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 12.0/38.1 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 15.0/38.1 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 17.5/38.1 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 18.5/38.1 MB 46.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 19.6/38.1 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 20.8/38.1 MB 32.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 22.3/38.1 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 23.8/38.1 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 25.5/38.1 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.0/38.1 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 29.8/38.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.3/38.1 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 33.9/38.1 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.8/38.1 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.1/38.1 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.1/38.1 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.1/38.1 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.1/38.1 MB 24.2 MB/s eta 0:00:00\n",
      "Installing collected packages: slicer, numpy, llvmlite, scipy, numba, shap\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.40.0\n",
      "    Uninstalling llvmlite-0.40.0:\n",
      "      Successfully uninstalled llvmlite-0.40.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.1\n",
      "    Uninstalling scipy-1.11.1:\n",
      "      Successfully uninstalled scipy-1.11.1\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.57.1\n",
      "    Uninstalling numba-0.57.1:\n",
      "      Successfully uninstalled numba-0.57.1\n",
      "Successfully installed llvmlite-0.45.1 numba-0.62.1 numpy-2.3.5 scipy-1.16.3 shap-0.50.0 slicer-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "686c5c81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"C:\\Users\\AlessiaDerossi\\anaconda3\\python.exe\"\n  * The NumPy version is: \"1.24.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\_core\\__init__.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\_core\\multiarray.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath, overrides\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\shap\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_explanation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cohorts, Explanation\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# explainers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m other\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\shap\\_explanation.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\cluster\\__init__.py:27\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=========================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mClustering package (:mod:`scipy.cluster`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhierarchy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vq, hierarchy\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PytestTester\n\u001b[0;32m     30\u001b[0m test \u001b[38;5;241m=\u001b[39m PytestTester(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\cluster\\vq.py:70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_asarray, array_namespace, is_lazy_array,\n\u001b[0;32m     71\u001b[0m                                    xp_capabilities, xp_copy, xp_size)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, rng_integers,\n\u001b[0;32m     73\u001b[0m                               _transition_to_rng)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\_lib\\_array_api.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Literal, TypeAlias\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnpt\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     is_array_api_obj,\n\u001b[0;32m     27\u001b[0m     is_lazy_array,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict\n\u001b[0;32m     37\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\typing\\__init__.py:160\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m============================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTyping (:mod:`numpy.typing`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# NOTE: The API section will be appended with additional entries\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# further down in this file\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# pyright: reportDeprecated=false\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrayLike, DTypeLike, NBitBase, NDArray\n\u001b[0;32m    162\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrayLike\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDTypeLike\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNBitBase\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDArray\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    165\u001b[0m __DIR \u001b[38;5;241m=\u001b[39m __all__ \u001b[38;5;241m+\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m k\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\_typing\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Private counterpart of ``numpy.typing``.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_like\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrayLike \u001b[38;5;28;01mas\u001b[39;00m ArrayLike\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_like\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NDArray \u001b[38;5;28;01mas\u001b[39;00m NDArray\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_like\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ArrayLike \u001b[38;5;28;01mas\u001b[39;00m _ArrayLike\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\_typing\\_array_like.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m     StringDType \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mStringDType\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# at runtime outside of type checking importing this from numpy.dtypes\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# would lead to a circular import\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDType\n\u001b[0;32m     19\u001b[0m _T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m _ScalarT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ScalarT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mgeneric)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\_core\\__init__.py:48\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m (sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m], sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m1\u001b[39m], sys\u001b[38;5;241m.\u001b[39mexecutable,\n\u001b[0;32m     47\u001b[0m         __version__, exc)\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m envkey \u001b[38;5;129;01min\u001b[39;00m env_added:\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"C:\\Users\\AlessiaDerossi\\anaconda3\\python.exe\"\n  * The NumPy version is: \"1.24.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado.\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
